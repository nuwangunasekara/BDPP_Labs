{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDPP Course lab 3 (Resilient Distributed Datasets)\n",
    "\n",
    "Welcome to the third lab of BDPP course! \n",
    "\n",
    "Here is a summary of what we will cover in this lab:\n",
    "\n",
    "- Work with the SparkSession objects\n",
    "- Speed benchmarking\n",
    "- Work with Resilient Distributed Datasets\n",
    "\n",
    "**Note:- We assume you are using pyspark on ALREADY created conda environment in LAB 1. If you have missed it, please refer to LAB 1.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SparkSession\n",
    "\n",
    "PySpark applications start with initializing `SparkSession` which is the entry point of PySpark as below. In case of running it in PySpark shell via <code>pyspark</code> executable, the shell automatically creates the session in the variable <code>spark</code> for users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T09:46:25.426635Z",
     "start_time": "2025-03-26T09:46:25.354770Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the Spark version\n",
    "Check the version of the Spark driver application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous spark versions used `SparkContext` and then created `SparkSession`s.\n",
    "If you need to access `SparkContext` through SparkSession use `sparkContext` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the functionality available with `sparkContext` are also available in `sparkSession`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Speed benchmark (Spark VS Pandas)\n",
    "\n",
    "Now, let's use the `sparkSession` object to run a simple benchmark by comparing reading a relatively big CSV file with pandas VS Spark. Although we are still running Spark on our local computer, it ends up reading the file faster than pandas (note that the result highly depends on the parallelization capabilities of your CPU). This demonstrates how Spark dataframes are much faster when compared to their pandas equivalent.\n",
    "\n",
    "For this experiment, we use a somewhat large Vermont vendor dataset. This data is accessible through [this link](https://data.vermont.gov/Finance/Vermont-Vendor-Payments/786x-sbp3). On this link, please select export and then choose CSV format. Download the file rename it to `Vermont_Vendor_Payments.csv` and place it in the `files` folder next to this notebook. Now, run the following two code snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "# loading csv file with Spark\n",
    "housing = spark.read.csv(\"files/Vermont_Vendor_Payments.csv\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installl pandas library if you don't have it.\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "# loading csv file with Pandas\n",
    "df_pandas = pd.read_csv(\"files/Vermont_Vendor_Payments.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question1: Use the cell below to report your observations from this experiment and compare the run times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" TODO_1: change the format of this cell to Markdown and answer the question here \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Speed benchmark ($\\pi$ calculation)\n",
    "\n",
    "Spark can also be used for compute-intensive tasks. This code estimates $\\pi$ by \"throwing darts\" at a circle. \n",
    "We pick random points in the unit square ((0, 0) to (1,1)) and see how many fall in the unit circle. \n",
    "The fraction should be $\\pi / 4$, so we use this to get our estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ï€ calculation code\n",
    "\n",
    "import random\n",
    "\n",
    "num_samples = 10000000 # you can change this number, e.g. try 100000000\n",
    "\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1 # if the point is inside the circle return True\n",
    "\n",
    "def spark_pi_calc():\n",
    "    # here we do the pi calcaulation using Spark\n",
    "    count = spark.sparkContext.parallelize(range(0, num_samples)).filter(inside).count() # count the number of points inside the circle\n",
    "    return (4.0 * count / num_samples) # return the estimated pi value\n",
    "\n",
    "def python_pi_calc():\n",
    "    # here we do the same calculation with python list comprehension\n",
    "    count = sum([inside(_) for throw in range(num_samples)])  # count the number of points inside the circle\n",
    "    return (4.0 * count / num_samples) # return the estimated pi value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on your hardware, this can take some time to fininsh.\n",
    "# You can reduce num_samples if it is taking too much time.\n",
    "\n",
    "print(\"[Spark] Pi is roughly:\", spark_pi_calc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on your hardware, this can take some time to fininsh.\n",
    "# You can reduce num_samples if it is taking too much time.\n",
    "\n",
    "print(\"[Python] Pi is roughly:\", python_pi_calc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use $\\pi$ calculation code to benchmark spark VS python. For small problems, python might work faster than spark because of the initial setup cost of spark. However, as the problem gets bigger, the spark code starts to show its benefit and runs faster than python.\n",
    "\n",
    "In the code below, we start from a small `num_samples` and keep doubling it until the python loop exceeds `max_time` (here it is set to 10 seconds by default). We collect running times for spark and python codes and produce a plot displaying time VS num_samples. \n",
    "\n",
    "We want you to play with the `max_time` parameter until the problem gets big enough so that you observe the spark code runs faster than the python code. This, of course, highly depends on the parallelization capacity of your CPU, and you may end up getting different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit, time\n",
    "\n",
    "max_time = 10 # you can also try 1, 2, 5, and 10 depending on your hardware performance.\n",
    "\n",
    "print('Running experiment. This may take a few minutes to run.')\n",
    "print('You can change max_time value to increase or decrease run time.')\n",
    "print('(please wait)')\n",
    "\n",
    "num_samples = 10000\n",
    "steps = []\n",
    "python_times = []\n",
    "sparks_times = []\n",
    "\n",
    "def my_timeit(func):\n",
    "    runs = 3  # If the experiment is still taking too much time to run, you may decrease this value as well.\n",
    "    dtime = timeit.timeit(func, number=runs)\n",
    "    elapsed = dtime/runs\n",
    "    return elapsed\n",
    "\n",
    "start = time.time()\n",
    "while True:\n",
    "    pt = my_timeit(python_pi_calc)\n",
    "    st = my_timeit(spark_pi_calc)\n",
    "    python_times.append(pt)\n",
    "    sparks_times.append(st)\n",
    "    steps.append(num_samples)\n",
    "    print(min(int(pt * 100), max_time*100), '/', max_time*100)\n",
    "    if pt > max_time:\n",
    "        break\n",
    "        \n",
    "    if pt > max_time:\n",
    "        break\n",
    "    elif pt < 0.1:\n",
    "        num_samples = num_samples * 10\n",
    "    else:\n",
    "        num_samples = num_samples * 2\n",
    "print(f\"Done! Total time = {time.time()-start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(steps, python_times, color='red', label='python')\n",
    "plt.plot(steps, sparks_times, color='blue', label='spark')\n",
    "plt.legend()\n",
    "plt.xlabel('number of samples', fontsize=12)\n",
    "plt.ylabel('running time (seconds)', fontsize=12)\n",
    "plt.title(r'Speed benchmark ($\\pi$ calculation)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question2: Use the cell below to report your observations from this experiment and compare the run times. Can Spark implementation run faster than Python on a single computer? How?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO_2: change the format of this cell to Markdown and answer the question here \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hint__: If you pick a big enough value for the `num_samples` parameter (code below), you should be able to see multiple python processes running at the same time in your (system monitor/task manager) when benchmarking spark code (A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code (A) Spark \n",
    "# Depending on your hardware, this can take some time to fininsh.\n",
    "# You can reduce num_samples if it is taking too much time.\n",
    "\n",
    "num_samples = 300000000  # reduce this number if it is taking too much time to run\n",
    "spark_pi_calc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code (B) Python - List Comprehension\n",
    "# Depending on your hardware, this can take some time to fininsh.\n",
    "# You can reduce num_samples if it is taking too much time.\n",
    "\n",
    "num_samples = 100000000  # reduce this number if it is taking too much time to run\n",
    "python_pi_calc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "__Question3__: Run the above experiment three times for each configuration and report the running times and mean and standard deviation for each setup. Discuss how adding extra resources affects the running time?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO_3: change the format of this cell to Markdown and answer the question here \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rdd\"></a>\n",
    "## 3. Work with Resilient Distributed Datasets\n",
    "Spark uses an abstraction for working with data called a Resilient Distributed Dataset (RDD). An RDD is a collection of elements that can be operated on in parallel. \\\n",
    "RDDs are **immutable**, so you can't update the data in them. To update data in an RDD, you must create a new RDD. In Spark, all work is done by creating new RDDs, transforming existing RDDs, or using RDDs to compute results. \n",
    "When working with RDDs, the Spark driver application automatically distributes the work across the cluster.\n",
    "\n",
    "You can construct RDDs by parallelizing existing Python collections (lists), by manipulating RDDs, or by manipulating files in HDFS or any other storage system.\n",
    "\n",
    "You can run these types of methods on RDDs: \n",
    " - Actions: query the data and return values\n",
    " - Transformations: manipulate data values and return pointers to new RDDs. \n",
    "\n",
    "Find more information on Python methods in the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html\" target=\"_blank\" rel=\"noopener noreferrer\">PySpark documentation</a>.\n",
    "\n",
    "<a id=\"rdd1\"></a>\n",
    "### 3.1 Create a collection\n",
    "Create a Python collection of the numbers 1 - 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rdd2\"></a>\n",
    "### 3.2 Create an RDD \n",
    "Put the collection into an RDD named `x_nbr_rdd` using the `parallelize` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nbr_rdd = spark.sparkContext.parallelize(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there's no return value. The `parallelize` method didn't compute a result, which means it's a transformation. Spark just recorded how to create the RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rdd3\"></a>\n",
    "### 3.3 View the data \n",
    "View the first element in the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nbr_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each number in the collection is in a different element in the RDD. Because the `first()` method returned a value, it is an action. \n",
    "\n",
    "Now view the first five elements in the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nbr_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "view all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nbr_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful with the `collect` method! It returns __all__ elements of the RDD to the driver. Returning a large data set might be not be very useful. No-one wants to scroll through a million rows!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rdd4\"></a>\n",
    "### 3.4 Create another RDD \n",
    "Create a Python collection that contains strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [\"Hello Human\", \"My Name is Spark\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the collection into an RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_str_rdd = spark.sparkContext.parallelize(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the first element in the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_str_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You created the string \"Hello Human\" and you returned it as the first element of the RDD. To analyze a set of words, you can map each word into an RDD element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"trans\"></a>\n",
    "## 4. Manipulate data in RDDs\n",
    "\n",
    "Remember that to manipulate data, you use transformation functions.\n",
    "\n",
    "Here are some common Python transformation functions that you'll be using in this notebook:\n",
    "\n",
    " - `map(func)`: returns a new RDD with the results of running the specified function on each element  \n",
    " - `filter(func)`: returns a new RDD with the elements for which the specified function returns true   \n",
    " - `distinct([numTasks]))`: returns a new RDD that contains the distinct elements of the source RDD\n",
    " - `flatMap(func)`: returns a new RDD by first running the specified function on all elements, returning 0 or more results for each original element, and then flattening the results into individual elements\n",
    "\n",
    "You can also create functions that run a single expression and don't have a name with the Python `lambda` keyword. For example, this function returns the sum of its arguments: `lambda a , b : a + b`.\n",
    "\n",
    "<a id=\"trans1\"></a>\n",
    "### 4.1 Update numeric values\n",
    "Run the `map()` function with the `lambda` keyword to replace each element, X, in your first RDD (the one that has numeric values) with X+1. Because RDDs are immutable, you need to specify a new RDD name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO_4: Replace <FILL IN> with appropriate code\n",
    "\n",
    "x_nbr_rdd_2 = x_nbr_rdd.map(lambda x: <FILL IN>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at the elements of the new RDD: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nbr_rdd_2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"trans2\"></a>\n",
    "### 4.2 Add numbers in an array\n",
    "An array of values is a common data format where multiple values are contained in one element. You can manipulate the individual values if you split them up into separate elements.\n",
    "\n",
    "Create an array of numbers by including quotation marks around the whole set of numbers. If you omit the quotation marks, you get a collection of numbers instead of an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [\"1,2,3,4,5,6,7,8,9,10\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an RDD for the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rd = spark.sparkContext.parallelize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the values at commas and add values in the positions 3 and 7 in the array. Keep in mind that an array starts with position 0. Use a backslash character, \\, to break the line of code for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO_5: Replace <FILL IN> with appropriate code\n",
    "\n",
    "Sum_rd = y_rd.map(lambda y: <FILL IN>).map(lambda y: <FILL IN>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now return the value of the sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sum_rd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get `12`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"trans3\"></a>\n",
    "### 4.3 Split and count text strings\n",
    "\n",
    "Create an RDD with a text string and show the first element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Words = [\"Hello Human. I'm Spark and I love running analysis on data.\"]\n",
    "words_rd = spark.sparkContext.parallelize(Words)\n",
    "words_rd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the string into separate lines at the space characters and look at the first element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO_6: Replace <FILL IN> with appropriate code\n",
    "\n",
    "Words_rd2 = words_rd.map(lambda line: <FILL IN>)\n",
    "Words_rd2.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of elements in this RDD with the `count()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Words_rd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get `1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you already knew that there was only one element because you ran the `first()` method and it returned the whole string. Splitting the string into multiple lines did not create multiple elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split the string again, but this time with the `flatmap()` method, and look at the first three elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO_7: Replace <FILL IN> with appropriate code\n",
    "\n",
    "words_rd2 = words_rd.flatMap(lambda line: <FILL IN>)\n",
    "words_rd2.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_rd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get `11`.\n",
    "This time each word is separated into its own element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"trans4\"></a>\n",
    "### 4.4 Count words with a pair RDD\n",
    "A common way to count the number of instances of words in an RDD is to create a pair RDD. A pair RDD converts each word into a key-value pair: the word is the key and the number 1 is the value. Because the values are all 1, when you add the  values for a particular word, you get the number of instances of that word.\n",
    "\n",
    "Create an RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = [\"First,Line\", \"Second,Line\", \"and,Third,Line\"]\n",
    "z_str_rdd = spark.sparkContext.parallelize(z)\n",
    "z_str_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the elements into individual words with the `flatmap()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_str_rdd_split_flatmap = z_str_rdd.flatMap(lambda line: line.split(\",\"))\n",
    "z_str_rdd_split_flatmap.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the elements into key-value pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countWords = z_str_rdd_split_flatmap.map(lambda word:(word,1))\n",
    "countWords.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now sum all the values by key to find the number of instances for each word: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "countWords2 = countWords.reduceByKey(add)\n",
    "countWords2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the word `Line` has a count of 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"filter\"></a>\n",
    "## 5. Filter data\n",
    "\n",
    "The filter command creates a new RDD from another RDD based on a filter criteria.\n",
    "The filter syntax is: \n",
    "\n",
    "`.filter(lambda line: \"Filter Criteria Value\" in line)`\n",
    "\n",
    "Hint: Use a simple python `print` command to add a string to your Spark results and to run multiple actions in single cell.\n",
    "\n",
    "Find the number of instances of the word `Line` in the `z_str_rdd_split_flatmap` RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO_8: Replace <FILL IN> with appropriate code\n",
    "\n",
    "words_rd3 = z_str_rdd_split_flatmap.filter(<FILL IN>) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_str_rdd_split_flatmap.map(lambda word:(word,1)).reduceByKey(add).filter(lambda word: word[0] == \"Line\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get `3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wordfile\"></a>\n",
    "## 6. Analyze text data from a file\n",
    "In this section, you'll use a text file `README.txt` to create an RDD from it, and analyze the text in it. The file should already exist on `files`folder next to this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wordfile2\"></a>\n",
    "### 6.2 Create an RDD from the file\n",
    "Use the `textFile` method to create an RDD named `textfile_rdd` based on the `README.txt` file. The RDD will contain one element for each line in the `README.txt` file.\n",
    "Also, count the number of lines in the RDD, which is the same as the number of lines in the text file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile_rdd = spark.sparkContext.textFile(\"files/README.txt\")\n",
    "textfile_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wordfile3\"></a>\n",
    "### 6.3 Filter for a word \n",
    "Filter the RDD to keep only the elements that contain the word \"Spark\" with the `filter` transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO_9: Replace <FILL IN> with appropriate code\n",
    "\n",
    "Spark_lines = textfile_rdd.filter(<FILL IN>)\n",
    "Spark_lines.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see `'# Apache Spark'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of elements in this filtered RDD and present the result as a concatenated string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO_10: Replace <FILL IN> with appropriate code\n",
    "\n",
    "print (\"The file README.txt has \" + str(Spark_lines.<FILL IN>) + \\\n",
    "\" of \" + str(textfile_rdd.<FILL IN>) + \\\n",
    "\" Lines with word Spark in it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see `The file README.txt has 19 of 98 Lines with word Spark in it.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wordfile4\"></a>\n",
    "### 6.4 Count the instances of a string at the beginning of words\n",
    "Count the number of times the substring \"Spark\" appears at the beginning of a word in the original text.\n",
    "\n",
    "Here's what you need to do: \n",
    "\n",
    "1. Run a `flatMap` transformation on the Spark_lines RDD and split on white spaces.\n",
    "2. Create an RDD with key-value pairs where the first element of the tuple is the word and the second element is the number 1.\n",
    "3. Run a `reduceByKey` method with the `add` function to count the number of instances of each word.<br>\n",
    "4. Filter the resulting RDD to keep only the elements that start with the word \"Spark\". In Python, the syntax to determine whether a string starts with a token is: `string.startswith(\"token\")` \n",
    "5. Display the resulting list of elements that start with \"Spark\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO_11: write your code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see:<br>\n",
    "<pre>\n",
    "[('Spark', 14),\n",
    " ('Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1),\n",
    " ('SparkPi', 2),\n",
    " ('Spark](#building-spark).', 1),\n",
    " ('Spark.', 1)]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wordfile5\"></a>\n",
    "### 6.5 Count instances of a string within words\n",
    "Now filter and display the elements that contain the substring \"Spark\" anywhere in the word, instead of just at the beginning of words like the last section. Your result should be a superset of the previous result.\n",
    "\n",
    "The Python syntax to determine whether a string contains a particular token is: `\"token\" in string`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO_12: write your code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see:\n",
    "<pre>\n",
    "[('Spark', 14),\n",
    " ('Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1),\n",
    " ('SparkPi', 2),\n",
    " ('Spark](#building-spark).', 1),\n",
    " ('Spark.', 1),\n",
    " ('tests](https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-AutomatedTesting).',\n",
    "  1)]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"numfile\"></a>\n",
    "## 7. Analyze numeric data from a file\n",
    "You'll analyze a sample file `Scores.txt` given in `files` folder that contains instructor names and scores. The file has the following format: Instructor Name,Score1,Score2,Score3,Score4,... The number of scores for each instructor could be diferent.\n",
    "Here is an example line from the text file: \"Carlo,5.5,3,3,4\" or \"Pablo,9,10,8.6,7,9,5,6\"\n",
    "Your task is to look at all the scores from each instructor and find the maximum score given by each instructor:\n",
    "\n",
    "1. Load the text file into an RDD.\n",
    "1. Run a transformation to create an RDD with the instructor names and the scores per instructor.\n",
    "1. Run a second transformation to compute the maximum score for each instructor. (you may have to convert each score to a float since original value is a string)\n",
    "1. Display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO_13: write your code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see:\n",
    "<pre>\n",
    "[('Tobias', 8.0),\n",
    " ('Malin', 10.0),\n",
    " ('Ali', 8.7),\n",
    " ('Magnus', 5.0),\n",
    " ('Alice', 9.1),\n",
    " ('Jack', 7.4)]\n",
    "</pre>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
